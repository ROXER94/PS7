{
 "metadata": {
  "name": "ps7.ipynb",
  "signature": "sha256:d0782989c0552dc9475995e09baed3d410f285d6cf0871c697e919e3e4bc484e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem Set 7 Notes\n",
      "\n",
      "##Problem 1:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The movielens dataset contains real movie preference data used for a variety of algorithmic tests (There are other datasets available as well at http://grouplens.org/datasets/ .)\n",
      "In this case I've retrieved from the smallest one (see [README](http://files.grouplens.org/datasets/movielens/ml-100k/README)) at this url<br>\n",
      "http://files.grouplens.org/datasets/movielens/ml-100k/<br>\n",
      "just two of the files:\n",
      "\n",
      "    u.item: list containing movie ids and titles (plus other info)\n",
      "    u.data: contains ratings  (user id, movie id, rating, timestamp)\n",
      "\n",
      "These two files are available at https://courses.cit.cornell.edu/info2950_2014fa/resources/ml.zip\n",
      "\n",
      "The first line of `u.item` has '|'-delimited fields, starting with\n",
      "\n",
      "    1|Toy Story (1995)|01-Jan-1995|...\n",
      "    ...\n",
      "\n",
      "Only the first two fields will be used here, so it can be loaded into a `movies` dict as\n",
      "\n",
      "    movies={}\n",
      "    for line in open('u.item'):\n",
      "        (id,title)=line.split('|')[0:2]\n",
      "        movies[int(id)] = title\n",
      "\n",
      "The file `u.data` has a list of tab-delimited user_id/movie_id/rating/timestamp lines\n",
      "\n",
      "    196     242     3       881250949\n",
      "    186     302     3       891717742\n",
      "    ...\n",
      "\n",
      "and can be loaded into a `ratings` dict as\n",
      "\n",
      "    ratings=defaultdict(dict)\n",
      "    for line in open('u.data'):\n",
      "        (user,movieid,rating,ts)=line.split('\\t')\n",
      "        ratings[user][movies[movieid]]=float(rating)\n",
      "        \n",
      "The objective is\n",
      "\n",
      "A. pick a few users, and get movie recommendations for them using, e.g.,  the `getRecommendations()` code from class in \n",
      "[lec22.ipynb](http://nbviewer.ipython.org/url/courses.cit.cornell.edu/info2950%5F2014fa/resources/lec22.ipynb), using the Pearson similarity measure.\n",
      "\n",
      "B. pick a few movies, and find movies similar to them using, e.g., the `topMatches()` code.\n",
      "\n",
      "The problem set will be updated with a few examples to determine whether it's coming out right. (This problem is easy since most of the code is already given, but still worthwhile since it uses real world data, and you'll have to think a bit about the code to see how it works and understand a bit better what was covered in class from the notebook.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 4:\n",
      "\n",
      "\n",
      "For the human genomic data, I went to http://www.ncbi.nlm.nih.gov/genome/guide/human/ and clicked on the first chromosome in the graphic in the left margin. That took me to a page which showed 20 genes labeled of the total of 3570 genes on chromosome 1. I then downloaded the sequence data for those 20 genes, and their lengths were:\n",
      "\n",
      "    [83637, 156021, 20374, 1472, 41520, 3323, 216842, 12682, 5950, 57943, 4407, 57544, 2301, 74578, 7829, 8616, 95627, 4892, 47410, 12068]\n",
      "\n",
      "for a total of 915036 bases. The first one \"Homo sapiens tumor protein p73 (TP73)\" starts 'AGGGGACGCAGCGAAACCGGGGC ... '.\n",
      "\n",
      "I attached them all together as a string `seq` and saved:\n",
      "\n",
      "    with gzip.open('seq.txt.gz','w') as gs: gs.write(seq)\n",
      "\n",
      "The file is available at\n",
      "https://courses.cit.cornell.edu/info2950_2014fa/resources/seq.txt.gz\n",
      "\n",
      "and if you've saved to disk can be loaded using (see note below if via wakari):\n",
      "\n",
      "    seq=gzip.open('seq.txt.gz').read()\n",
      "\n",
      "The \"unigram\" and \"bigram\" counts can be obtained as above for the text corpus:\n",
      "\n",
      "    ucount=Counter(seq)\n",
      "    bcount=Counter([(seq[i],seq[i+1]) for i in range(len(seq)-1)])\n",
      "\n",
      "and more generally the n-gram counts can be obtained using\n",
      "\n",
      "    ncount=Counter([tuple(seq[i:i+n]) for i in range(0,len(seq)-n)])\n",
      "\n",
      "Any of these can be converted to probability arrays by dividing by the sum, e.g.:\n",
      "\n",
      "    nprob=array(ncount.values())/float(sum(ncount.values()))\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}